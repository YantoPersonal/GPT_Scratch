{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GPT Development",
   "id": "d7e77b4176bc21be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Statements",
   "id": "7ff3ef1764d336b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:45.609003Z",
     "start_time": "2024-06-24T09:03:34.546476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "8b7094ce682692fc",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reading Data",
   "id": "7ecc59ffa6742189"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:45.642864Z",
     "start_time": "2024-06-24T09:03:45.610044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reading and Viewing the Dataset:\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(f\"The Length of the text: {len(text)}\")\n",
    "print(text[:1000])"
   ],
   "id": "60a2802bc565f48e",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenization\n",
    "\n",
    "- Encodes text into integers, using a different vocabulary, and is a sub-word tokenizer: https://github.com/google/sentencepiece\n",
    "- OpenAI utilises tiktoken - https://github.com/openai/tiktoken\n",
    "\n",
    "```\n",
    "# OpenAI - tiktoken package example:\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "enc.nvocab \n",
    "# Output ---> 50257\n",
    "\n",
    "enc.encode(\"hii there\") \n",
    "#Output ---> [71, 4178, 612]\n",
    "enc.decode([71, 4178, 612]) \n",
    "# Output ---> \"hii there\"\n",
    "```\n",
    "There's a trade-off between vocabulary size & encoding size. In Open AI's example, we see that they have a vocabulary size of 50,257 items. When compared to a simple, character to integer model, we have 65 (you can see the vocabulary below). As such, OpenAI is storing more upfront memory compared to us. \n",
    "\n",
    "However, there are significant benefits to this in the long run, notice, that the output to encode the string \"hii there\" is only a vector of 3 items. Whereas our encoding for such a problem would be 8 items long. That's a significant difference in length that will have an impact on model performance."
   ],
   "id": "527c858ba8ebac31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:45.684729Z",
     "start_time": "2024-06-24T09:03:45.645333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Unique Characters and Vocabulary Size:\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"The Length of the vocabulary: {vocab_size}\")\n",
    "print(''.join(chars))"
   ],
   "id": "b53e8c57254809fe",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:46.120724Z",
     "start_time": "2024-06-24T09:03:45.686281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenization - Create a Mapping from Characters to Integers.\n",
    "\n",
    "# (1) Generate Dictionaries for Mapping\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# (2) Design Mapping Functions\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join(itos[l] for l in l)\n",
    "\n",
    "# (3) Test Functions: \n",
    "print(encode(\"The quick brown fox jumps over the lazy dog\"))\n",
    "print(decode(encode(\"The quick brown fox jumps over the lazy dog\")))\n",
    "\n",
    "# (4) Encode the Entire Dataset:\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"The Shape of the Tensor is: {data.shape}\\nThe Type of the Shape is: {data.dtype}\")\n",
    "print(\"Data:\", data[0:100])\n",
    "\n",
    "# (5) Train-Test Split:\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "a3a3bc9e07b34d9a",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loader\n",
    "\n",
    "The Data Loader is designed such that we're able to select randomized chunks of the source dataset (in batches) and train the dataset on that specific batch. Then once training on the batch is completed, select another randomized batch to train.\n",
    "\n",
    "Unlike an LSTM, where you will take in a series of values and look to predict a single following value e.g: `x_train = [5, 4, 3, 2]; y_pred = 1` takes a sequence of 4 characters to predict the next character. Transformers, make predictions at each step, using the information at that step and what came previously. For example given known data: `data = [5, 4, 3, 2, 1]` we would like to make the following prediction:\n",
    "\n",
    "```\n",
    "x_train = [5]; y_pred = 4\n",
    "x_train = [5, 4]; y_pred = 3\n",
    "x_train = [5, 4, 3]; y_pred = 2\n",
    "x_train = [5, 4, 3, 2]; y_pred = 1\n",
    "```\n",
    "For a given block size, infers how many training samples you have from a single block.\n"
   ],
   "id": "ca03364d41921a65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:46.141915Z",
     "start_time": "2024-06-24T09:03:46.123799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transformer: Demonstration of Train vs. Target\n",
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Input: {context}, Target: {target}\")"
   ],
   "id": "d5d0fca7c5f0c99f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:46.207280Z",
     "start_time": "2024-06-24T09:03:46.144502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# (0) Define Batch Loader:\n",
    "def get_batch(split, debug=False):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    print(f\"The split used is {split}: \\n{data[:100]}\") if debug else None\n",
    "    \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    print(f\"\\nThe random integers generated: {ix}\") if debug else None\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    print(f\"\\nSampling from the dataset sequentially from starting point(s) ix: \\n{torch.stack([data[i:i+block_size] for i in ix])}\") if debug else None\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# (1) Return Batch Data:\n",
    "xb, yb =  get_batch('train', debug=True)\n",
    "\n",
    "# (2) Unpack Batch Data: \n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"Input: {context.tolist()}, Target: {target}\")"
   ],
   "id": "f5ba6cb7df4aa37",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bigram Language Model\n",
    "The simplest language model that we can implement, it was covered in the makemore series here: https://www.youtube.com/watch?v=PaCmpygFfXo. Remember that from a Bigram Model, we are essentially creating a table that determines the probability of the *next character*, given the *previous character*. Each row, is converted into a probability distribution using the softmax function to ensure that the sum of each row is equal to 1. In PyTorch, we are using the *nn.Embedding* layer that is already pre-defined for us.\n",
    "\n",
    "\n",
    "**Input Data:**\n",
    "```\n",
    "batch_size = 4 # Denoted by B.\n",
    "n_characters = 8 # Denoted by T.\n",
    "\n",
    "print(xb.shape) # Output --> (4,8)\n",
    "print(yb.shape) # Output --> (4,8)\n",
    "\n",
    "# Hence xb & yb are of shape (B,T)\n",
    "```\n",
    "\n",
    "**Embedding Table:**\n",
    "```\n",
    "vocab_size = 65 # Denoted as C\n",
    "self.token_embedding_table # Embedding (65, 65) or (C, C).\n",
    "```\n",
    "\n",
    "**Multiplication:**\n",
    "Intuitively, we have a tensor of $(B, T)$ which for each $B$ contains $T$ indices we'd like to select from the embedding table. Once selected, each $T$ is represented by the logits of size $C$.\n",
    "\n",
    "However, for this multiplication to work. We need to one-hot our indices values (one hot of size vocab size). Such that we have a $B, T, C$ when mulitplied by $C, C$ will give us a $B, T, C$, with the values being from the embedding table for the specified one-hot row."
   ],
   "id": "b01b4c2b36566a85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:46.247510Z",
     "start_time": "2024-06-24T09:03:46.207280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Demonstration of Matrix Multiplication:\n",
    "temp1 = torch.tensor([[3, 2, 13, 25, 45, 21, 54, 7]])\n",
    "t2 = torch.randint(0, 10, (65,65))\n",
    "print(temp1.shape, t2.shape)\n",
    "\n",
    "# One-Hot temp1 by vocab size:\n",
    "t1 = F.one_hot(temp1, vocab_size) # Now (B, T, C)\n",
    "print(f\"One hot Vector: \\n{t1[0, 0]}\") # For the value 3, we can see the resulting one hot vector.\n",
    "\n",
    "# Multiplication:\n",
    "tlogits = t1 @ t2\n",
    "\n",
    "print(f\"Output for Resulting idx = 3: \\n{tlogits[0,0]}\")\n",
    "print(f\"The learned logits for idx = 3: \\n{t2[3]}\")"
   ],
   "id": "692b37ceca707fd5",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:46.270296Z",
     "start_time": "2024-06-24T09:03:46.248532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        :param idx: matrix of integers corresponding to characters. \n",
    "        :param targets: matrix of integers corresponding to target characters.\n",
    "        :return logits: associated probabilities depending on input character.\n",
    "        :return loss: associated loss based on logits vs. known target.\n",
    "        \"\"\"\n",
    "        \n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:        \n",
    "            B, T, C = logits.shape\n",
    "            \n",
    "            \n",
    "            # LOADS OF TRANSFORMER SHIT\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets) # loss doesn't work with batch B, so flattened logits & targets.\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        :param idx: matrix of integers corresponding to characters.\n",
    "        :param max_new_tokens: number of tokens to generate.\n",
    "        :return: generated characters from bi-gram model.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(idx)\n",
    "            logits = logits[:, -1, :] # Final time-step (letter), (B,C).\n",
    "            probs = F.softmax(logits, dim=-1) # logits to probability dist.\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=-1) # (B, T+1)\n",
    "        return idx"
   ],
   "id": "ab200fdb60d9eb16",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:46.330022Z",
     "start_time": "2024-06-24T09:03:46.272757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss) # True loss -ln(1/65)"
   ],
   "id": "6323080d12edf51a",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:03:46.404449Z",
     "start_time": "2024-06-24T09:03:46.331933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate new Samples:\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ],
   "id": "87a140b8d8a3dbdd",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.723490Z",
     "start_time": "2024-06-24T09:03:46.407477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training the bi-gram model:\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())\n",
    "    "
   ],
   "id": "a6e7ba7cd49401",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.781896Z",
     "start_time": "2024-06-24T09:05:09.727012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate new Samples (Trained):\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ],
   "id": "8b3b25af978474ac",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Mathematics of Self-Attention",
   "id": "1457fe719ade3a24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Toy Example:",
   "id": "7e5ef883dc5597dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the following example, we have data from two batches (samples). Each sample contains 8 tokens (in this case, tokens for us are individual characters), which are embedded into 2 channels (each token, which consists of a single value, is represented as a vector of size 2).",
   "id": "e8a5328f0afe8e8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.817003Z",
     "start_time": "2024-06-24T09:05:09.782773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ],
   "id": "b3217d67bf32c54e",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So far we've just been prediction the next token based on the previous token. Currently, we don't have tokens communicating with each other. Our communication, needs to be setup in such a way that \"future\" tokens can't communicate with \"previous\" tokens. In other words, the present can communicate with the past, but not the future. E.g. with 8 tokens. token at position 5 should be able to communicate with those in position 4, 3, 2, 1 & 0. But not position 6!\n",
    "\n",
    "What we will set up:\n",
    "For every token calulate the average of all the channel vectors in the current token and all previous tokens.\n",
    "Attention for Token 5 = ([C5_1, C5_2] + [C4_1, C4_2] + [C3_1, C3_2] ... [C0_1, C0_2])/5 \n"
   ],
   "id": "47e5fc92c015d5c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.841975Z",
     "start_time": "2024-06-24T09:05:09.825353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ],
   "id": "4c6c05ef5d0da32f",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.857784Z",
     "start_time": "2024-06-24T09:05:09.846016Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Viewing only the first batch of data: \\n{x[0]}\")",
   "id": "766f8109c3f7c3da",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.881243Z",
     "start_time": "2024-06-24T09:05:09.859805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for t_select in range(T):\n",
    "    print(f\"For batch zero. {xbow[0,t_select]} is the average of all token-channel representations less than or equal to {t_select}.\")"
   ],
   "id": "53f1127a23e651b7",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mathematically, the above can be verified:\n",
    "- xbow [0,0] = [(0.1808)/1, (-0.0700)/1] = [0.1808, -0.0700]\n",
    "- xbow [0,1] = [(0.1808 + -0.3596)/2, (0.0700 + -0.9152)/2] = [-0.0894, -0.4926]\n",
    "\n",
    "Essentially: $X_{b}[B,T] = [(x_{b=B,t=1,c=1} + ... + x_{b=B,t=T,c=1})/C,..., (x_{b=B,t=1,c=C} + ... + x_{b=B,t=T,c=C})/C)] $"
   ],
   "id": "135429959b03c1d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The only improvments from here, look to make the above operation into entirely a matrix multiplication operation, instead of using for loops. Otherwise, principly we are doing the exact same thing.",
   "id": "b53b3251163c848f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.899762Z",
     "start_time": "2024-06-24T09:05:09.885106Z"
    }
   },
   "cell_type": "code",
   "source": "F.sigmoid(xbow[0])",
   "id": "38a2c80848c6e775",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Understanding the Matrix Multiplication Trick:",
   "id": "25155dc176d4acaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.938494Z",
     "start_time": "2024-06-24T09:05:09.902873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones((3,3), requires_grad=False)) # <-- Not in notes about requires grad, but this would never be changed.\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(f\"The output of a is \\n{a}\")\n",
    "print(f\"The output b is \\n{b}\")\n",
    "print(f\"The output c is \\n{c}\")"
   ],
   "id": "faea5d160b8545e9",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Imagine that the matrix $b$ is a matrix with three tokens $T$ and two channels $C$. By making a triangular matrix $a$ of only ones, each output of matrix $c$ is the current token channels + previous token channels.\n",
    "\n",
    "- Row 0 X Col 0 = (1 * 2) + (0 * 6) + (0 * 6) = 2\n",
    "- Row 0 X Col 1 = (1 * 7) + (0 * 4) + (0 * 5) = 7\n",
    "\n",
    "Output of the first row of $c$ is [2, 7]\n",
    "\n",
    "- Row 1 X Col 0 = (1 * 2) + (1 * 6) + (0 * 6) = 8\n",
    "- Row 1 X Col 1 = (1 * 7) + (1 * 4) + (0 * 5) = 11\n",
    "\n",
    "Output of the first row of $c$ is [8, 11]\n",
    "\n",
    "- Row 2 X Col 0 = (1 * 2) + (1 * 6) + (1 * 6) = 14\n",
    "- Row 2 X Col 1 = (1 * 7) + (1 * 4) + (1 * 5) = 16\n",
    "\n",
    "Output of the first row of $c$ is [14, 16]\n",
    "\n",
    "The shapes of the matrix multiply work as such: $A * B$ = $(T, T) * (T, C) = (T, C)$"
   ],
   "id": "4baab344ef98c57c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Adding Mean:",
   "id": "96cb53b60c2360cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:09.971086Z",
     "start_time": "2024-06-24T09:05:09.938494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones((3,3), requires_grad=False)) # <-- Not in notes about requires grad, but this would never be changed.\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "c = a @ b\n",
    "print(f\"The output of a is \\n{a}\")\n",
    "print(f\"The output b is \\n{b}\")\n",
    "print(f\"The output c is \\n{c}\")"
   ],
   "id": "ddb8a802c9123bc5",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The only difference with the above, is that now $a$ can also handle the average operation. As for each token, its respective channel is represented equally out of the total:\n",
    "\n",
    "- Row 0 X Col 0 = (1 * 2) + (0 * 6) + (0 * 6) = 2\n",
    "- Row 0 X Col 1 = (1 * 7) + (0 * 4) + (0 * 5) = 7\n",
    "\n",
    "Output of the first row of $c$ is [2, 7]\n",
    "\n",
    "- Row 1 X Col 0 = (0.5 * 2) + (0.5 * 6) + (0 * 6) = 4\n",
    "- Row 1 X Col 1 = (0.5 * 7) + (0.5 * 4) + (0 * 5) = 5.5\n",
    "\n",
    "Output of the first row of $c$ is [4, 5.5]\n",
    "\n",
    "- Row 2 X Col 0 = (0.33 * 2) + (0.33 * 6) + (0.33 * 6) = 4.6667\n",
    "- Row 2 X Col 1 = (0.33 * 7) + (0.33 * 4) + (0.33 * 5) = 5.3333\n",
    "\n",
    "Output of the first row of $c$ is [4.6667, 5.3333]\n",
    "\n"
   ],
   "id": "88608694c63dc695"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.013961Z",
     "start_time": "2024-06-24T09:05:09.973603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "wei = torch.tril(torch.ones((T,T), requires_grad=False)) # <--- Will be True! Because, the model will shift around these value, to give \"importance\" to some relationships.\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "print(f\"Our Mean Calculation Matrix looks like: \\n{wei}\")\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "xbow2 = wei @ x # before we did (T, T) @ (T, C) --> (T, C)\n",
    "# (T, T) @ (B, T, C) doesn't work so it auto's to (B, T, T) @ (B, T, C) ---> (B, T, C)\n",
    "\n",
    "xbow[0], xbow2[0] # Same as our first example."
   ],
   "id": "75ecda6d5df74f0e",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Softmax",
   "id": "52c0390d5bda9bed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.046234Z",
     "start_time": "2024-06-24T09:05:10.014989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wei = torch.tril(torch.ones((T,T), requires_grad=True))\n",
    "print(f\"Wei: \\n{wei}\")\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print(f\"Broken Softmax: \\n{wei}\")\n",
    "\n",
    "# Fix:\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T), requires_grad=True) # <--- These are like interaction strengths, initially are zero, but as the model learns will associate some characters together.\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # Present tokens can't comunicate with future, only the past.\n",
    "print(f\"Wei: \\n{wei}\")\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print(f\"Working Softmax: \\n{wei}\")"
   ],
   "id": "9ac098c728866d5b",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, we've replaced the A / A.sum with an activation fuction, softmax!\n",
    "\n",
    "Softmax: \n",
    "\n",
    "For a given vector: $z = (z_{1} ... z_{k})$\n",
    "\n",
    "The softmax output at position $i$: $o(z)_{i} = \\frac{e^{z_{i}}}{\\sum^{K}_{j=1} e^{z_j}$\n",
    "\n",
    "Important to note that: $e^0 = 1$ and $e^{-inf} = 0$\n",
    "\n",
    "Hence we can't just do our normal tril in our normal stages. We instead create the tril, and everywhere the tril is equal to zero, we convert to -inf in the wei matrix. The other locations are left the same, hence wei = 0.\n",
    "\n",
    "This is the fundamental math we will use to develop the **self-attention block**."
   ],
   "id": "b24baefe9ef2a6f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.063230Z",
     "start_time": "2024-06-24T09:05:10.049207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This would then be multiplied by an appropriate (T, C) Matrix.\n",
    "xbow3 = wei @ x\n",
    "print(xbow3.shape)\n",
    "print(xbow3[0])"
   ],
   "id": "640b7a174c392e83",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the above, [1808, -0.0700] is the weighting of the 1st Token by itself. [-0.0894, -0.4926] is the weighting of the 2nd Token with itself and the first. Finally [-0.0341, 0.1332] is the weighting of the 8th token with all previous tokens. However, rather than just being a simple average as before (it currently is), the weights in wei can be manipulated to make certain tokens more or less important.\n",
    "\n",
    "This is the preview of self-attention. You can do weighted aggregation of past values, and how much of each element fuses into each position =)"
   ],
   "id": "62b25b528bcf6e80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Crux of Self Attention\n",
    "This section is completed at time (1:02:08) after the v2-1 Positional Encoding.py file.\n",
    "\n"
   ],
   "id": "8ba54cd7b8362613"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.108879Z",
     "start_time": "2024-06-24T09:05:10.067022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We have a Batch Size of B, with a block size of T (n_token) and each token is represented in a vector of size C.\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T)) # Initialized as Equal. ==> To be learned through data. How?\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(f\"Weights before softmax: \\n{wei}\")\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print(f\"Weights after softmax: \\n{wei}\")\n",
    "\n",
    "out = wei @ x\n",
    "print(f\"Final Output {out.shape}: \\n{out[0][0]}\") # Visualize a Single Attention for Batch Zero, Position 0."
   ],
   "id": "2f126a8c02988635",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Every Single Token emits two Vectors; a Query and a Key. The Query Vector roughly translates to \"What Am I Looking For?\", while the Key Vector translates to \"What values do I contain?\". The way we then gather relationships between tokens in a sequence, is say that we are at Token (T). We take its Query vector and multiply it by all of the key vectors, getting a single value that higher when those two things match. That dot product is what becomes the matrix *wei*.",
   "id": "84429d53570ce0d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.157241Z",
     "start_time": "2024-06-24T09:05:10.108879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We have a Batch Size of B, with a block size of T (n_token) and each token is represented in a vector of size C.\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Implement one \"Head\":\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # (32, 16)\n",
    "query = nn.Linear(C, head_size, bias=False) # (32,16)\n",
    "\n",
    "# Each Token (which is embedded as a vector of 32, is matmul with the key and query layers to make k, q).\n",
    "k = key(x) # (4, 8, 16)\n",
    "q = query(x) # (4, 8 ,16)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (4, 8, 16) @ (4, 16, 8) ---> (B, 8, 8) or (B, T, T). (ignore, the B and see how they would multiply). (8, 16) @ (16, 8) = (8, 8)\n",
    "\n",
    "print(f\"Shape of k: \\n{k.shape}\")\n",
    "print(f\"Shape of q: \\n{q.shape}\")\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(f\"Weights before softmax (for Query 0): \\n{wei[0]}\")\n",
    "wei = F.softmax(wei, dim=1) # Probability Distribution raw values.\n",
    "print(f\"Weights after softmax (for Query 0): \\n{wei[0]}\")\n",
    "# Wei is now controlled by the key and query values, the larger the value the more it contributes to the final output at out.\n",
    "\n",
    "out = wei @ x\n",
    "print(f\"Final Output {out.shape}: \\n{out[0][0]}\") # Visualize a Single Attention for Batch Zero, Position 0."
   ],
   "id": "1c4c2b6fa0e796c4",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have created a single \"head\" that implements our ideas. Each Token vector of 32, is used to make a key and a query as described above. We do this in parallel by taking our data x (B, T, C) and a matrix (C, C/2). The B & T are treated like batch dimensions and we return (B, T, C/2) for each.\n",
    "\n",
    "Then we calculate wei. Such that for each row (query), is the multiplied by each key. We are left with a weight matrix which can be read as follows.\n",
    "wei[0,0] = How much does the query of Token 0 match the key of Token 0?\n",
    "wei[3,0] = How much does the query of Token 3 match the key of Token 0?\n",
    "\n",
    "wei[1, 3] = How much does the query of Token 1 match the key of Token? However, remember this is reading into the future, and so when we apply our mask, this will not be possible."
   ],
   "id": "d66d93692e7de1ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.196143Z",
     "start_time": "2024-06-24T09:05:10.161197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We have a Batch Size of B, with a block size of T (n_token) and each token is represented in a vector of size C.\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Implement one \"Head\":\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # (32, 16)\n",
    "query = nn.Linear(C, head_size, bias=False) # (32, 16)\n",
    "value = nn.Linear(C, head_size, bias=False) # (32, 16)\n",
    "\n",
    "# Each Token (which is embedded as a vector of 32, is matmul with the key and query layers to make k, q).\n",
    "k = key(x) # (4, 8, 16)\n",
    "q = query(x) # (4, 8 ,16)\n",
    "v = value(x) # The vector that gets multiplied by wei to make the output\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (4, 8, 16) @ (4, 16, 8) ---> (B, 8, 8) or (B, T, T). (ignore, the B and see how they would multiply). (8, 16) @ (16, 8) = (8, 8)\n",
    "\n",
    "print(f\"Shape of k: \\n{k.shape}\")\n",
    "print(f\"Shape of q: \\n{q.shape}\")\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(f\"Weights before softmax (for Query 0): \\n{wei[0]}\")\n",
    "wei = F.softmax(wei, dim=-1) # Probability Distribution raw values.\n",
    "print(f\"Weights after softmax (for Query 0): \\n{wei[0]}\")\n",
    "# Wei is now controlled by the key and query values, the larger the value the more it contributes to the final output at out.\n",
    "\n",
    "# out = wei @ x\n",
    "out = wei @ v\n",
    "print(f\"Final Output {out.shape}: \\n{out[0][0]}\") # Output is now only 16 m"
   ],
   "id": "892e561c2fe6c5b7",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We've now added a final vector value! Essentially the query matches the key, what we send is not x specifically, it a value vector that is associated to x. The motivation for this isn't massive clear to me why we don't just use raw x. That would require more research. But I can see how the value (v), is kept separate from the key and query essentially, and for all intensive purposes k & q do not know v exist, but they would know x exists. So this is perhaps why. \n",
    "\n",
    "It should also be clear that, batches don't communicate with one another, and we wouldn't want this, these are just parallel training operations. It can only use data within it's exact block (context window).\n",
    "\n",
    "In text generation, we block future tokens from communcating. However if you're not generating, perhaps you'd let all nodes to communicate (removing the masked fill), to for example determine the sentiment of a sentence. \n",
    "\n",
    "\"self attention\" - Why Self? It's self becuase the keys, queries and values all come from the same base information (x). Cross Attention = You could have the queries be produced from x, but they keys and values are produced from y. Your Query is in spanish perhaps, and you'd like to respond in English? I think of it like this.\n",
    "\n",
    "\"Scaled attention\" divides wei by 1/sqrt(head_size). This makes it so when inpur Q, K are unit variance, wei will be unit variance too and softmax will stay diffused and not saturate too much. This is similar to the kaeming normalization we seen before, where again we were aiming at unit gaussian (normal distrubion with mean 0 and std of 1 and so 2/3rds of the data is between -1 and + 1)."
   ],
   "id": "26d580506457133c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.209850Z",
     "start_time": "2024-06-24T09:05:10.199410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# scaled attention\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "wei_norm = q @ k.transpose(-2, -1) * head_size **-0.5"
   ],
   "id": "9bf204a0a6d99725",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.221930Z",
     "start_time": "2024-06-24T09:05:10.213212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"k variance: \\n{k.var()}\")\n",
    "print(f\"q variance: \\n{q.var()}\")\n",
    "print(f\"wei no norm variance: \\n{wei.var()}\")\n",
    "print(f\"wei norm: \\n{wei_norm.var()}\")"
   ],
   "id": "c18d16e214253ae2",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If softmax takes on too widely varying values, it will essentially become one-hot vectors.",
   "id": "99b183ec7a6633bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.238666Z",
     "start_time": "2024-06-24T09:05:10.225458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)) # Normalized\n",
    "print(torch.softmax(torch.tensor([100, -0.2, 0.3, -0.2, -10]), dim=-1)) # Not Normalized (Essentially, [1, 0, 0, 0, 0] one hot)."
   ],
   "id": "9e7236b947fbbe43",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.251554Z",
     "start_time": "2024-06-24T09:05:10.238666Z"
    }
   },
   "cell_type": "code",
   "source": "# Return at 1:19:20 for the programming of the above section from \"Crux of Self Attention\" into a class called Head!",
   "id": "7562b86f1df9dc62",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T09:05:10.260263Z",
     "start_time": "2024-06-24T09:05:10.255387Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c5399dae31c9801d",
   "execution_count": 29,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
